{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca04ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import string\n",
    "from string import punctuation\n",
    "import collections\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe485bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the file with the coded tweets\n",
    "df_1_new = pd.read_csv('scores_df1_full.csv')\n",
    "\n",
    "# new dataframe with relevant columns\n",
    "df1 = df_1_new[['Date', 'clean_no_stops', 'sentiment code', 'category']].copy()\n",
    "\n",
    "# subset with manually coded tweets\n",
    "subset_1 = df1[0:1235]\n",
    "subset_1.dropna(inplace=True)\n",
    "subset_1.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0704de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split words into lists\n",
    "v = subset_1['clean_no_stops'].str.split().tolist()\n",
    "# compute global word frequency\n",
    "c = Counter(chain.from_iterable(v))\n",
    "# filter, join, and re-assign\n",
    "subset_1['clean_new'] = [' '.join([j for j in i if c[j] > 3]) for i in v] # remove low frequency words\n",
    "\n",
    "subset_1[\"count\"] = \"\"\n",
    "for i in range(len(subset_1)):\n",
    "    subset_1['count'][i] = int(str(len(subset_1['clean_new'][i].split())))\n",
    "\n",
    "subset_1 = subset_1[subset_1['count'] > 5] # keep tweets with more than 5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9283a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual stemming to dominant words in the dataset\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('השחקן','שחקן').str.replace('השחקנים','שחקן')\n",
    "                            .str.replace('כשחקן','שחקן').str.replace('שחקני','שחקן').str.replace('לשחקן','שחקן')\n",
    "                            .str.replace('לשחקני','שחקן').str.replace('בשחקן','שחקן').str.replace('מהשחקנים','שחקן')\n",
    "                            .str.replace('ושחקנים','שחקן').str.replace('והשחקן','שחקן').str.replace('ישחק','שחקן'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('ערבים','ערבי').str.replace('הערבי','ערבי')\n",
    "                            .str.replace('הערבים','ערבי').str.replace('לערבים','ערבי').str.replace('והערבים','ערבי')\n",
    "                            .str.replace('שהערבים','ערבי').str.replace('בערבית','ערבי').str.replace('ערביי','ערבי')\n",
    "                            .str.replace('ערבית','ערבי').str.replace('וערבים','ערבי'))\n",
    "\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('במשחק','משחק').str.replace('המשחק','משחק')\n",
    "                            .str.replace('למשחק','משחק').str.replace('ומשחקים','משחק').str.replace('שמשחק','משחק')\n",
    "                            .str.replace('המשחקים','משחק').str.replace('ומשחק','משחק').str.replace('מהמשחק','משחק')\n",
    "                            .str.replace('שמשחקים','משחק').str.replace('משחקי','משחק').str.replace('למשחקים','משחק')\n",
    "                           .str.replace('משחקת','משחק').str.replace('ומשחקת','משחק').str.replace('במשחקים','משחק'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('בנבחרת','נבחרת').str.replace('הנבחרת','נבחרת')\n",
    "                            .str.replace('לנבחרת','נבחרת').str.replace('מהנבחרת','נבחרת').str.replace('שהנבחרת','נבחרת')\n",
    "                            .str.replace('כשהנבחרת','נבחרת').str.replace('בנבחרות','נבחרת'))\n",
    "\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('ישראלי','ישראל').str.replace('בישראל','ישראל')\n",
    "                            .str.replace('הישראלי','ישראל').str.replace('לישראל','ישראל').str.replace('ישראלים','ישראל')\n",
    "                            .str.replace('ישראלית','ישראל').str.replace('והישראלים','ישראל').str.replace('מהישראלים','ישראל')\n",
    "                           .str.replace('הישראלים','ישראל').str.replace('שישראלי','ישראל').str.replace('כשישראלי','ישראל')\n",
    "                           .str.replace('לישראלי','ישראל'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('שערים','שער').str.replace('השער','שער')\n",
    "                            .str.replace('לשער','שער').str.replace('השערים','שער').str.replace('משערי','שער')\n",
    "                            .str.replace('בשער','שער').str.replace('שלושער','שער').str.replace('משער','שער')\n",
    "                           .str.replace('מהשער','שער').str.replace('שערי','שער').str.replace('ששער','שער')\n",
    "                           .str.replace('ושער','שער'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('הקבוצה','קבוצה').str.replace('בקבוצה','קבוצה')\n",
    "                            .str.replace('לקבוצה','קבוצה').str.replace('שהקבוצה','קבוצה').str.replace('וקבוצה','קבוצה'))\n",
    "\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('דקות','דקה').str.replace('בדקה','דקה')\n",
    "                            .str.replace('מדקה','דקה').str.replace('בדקות','דקה').str.replace('לדקה','דקה')\n",
    "                            .str.replace('מהדקות','דקה'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('בעונה','עונה').str.replace('לעונה','עונה')\n",
    "                            .str.replace('מהעונה','עונה').str.replace('מעונה','עונה').str.replace('לדקה','דקה')\n",
    "                            .str.replace('מהדקות','דקה'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('במכבי','מכבי').str.replace('למכבי','מכבי')\n",
    "                            .str.replace('ממכבי','מכבי').str.replace('ומכבי','מכבי').str.replace('שמכבי','מכבי')\n",
    "                            .str.replace('מהדקות','דקה'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('במכבי','מכבי').str.replace('למכבי','מכבי')\n",
    "                            .str.replace('ממכבי','מכבי').str.replace('ומכבי','מכבי').str.replace('שמכבי','מכבי'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('במקום','מקום').str.replace('למקום','מקום')\n",
    "                            .str.replace('מקומות','מקום').str.replace('ובמקום','מקום').str.replace('מקומו','מקום')\n",
    "                            .str.replace('במקומם','מקום').str.replace('ממקום','מקום').str.replace('במקומות','מקום'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('במקום','מקום').str.replace('למקום','מקום')\n",
    "                            .str.replace('מקומות','מקום').str.replace('ובמקום','מקום').str.replace('מקומו','מקום')\n",
    "                            .str.replace('במקומם','מקום').str.replace('ממקום','מקום').str.replace('במקומות','מקום'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('הליגה','בליגה').str.replace('לליגה','בליגה')\n",
    "                            .str.replace('ליגה','בליגה').str.replace('הליגות','בליגה'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('ההרכב','הרכב').str.replace('בהרכב','הרכב')\n",
    "                            .str.replace('ההרכבים','הרכב').str.replace('הרכבים','הרכב').str.replace('להרכב','הרכב'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('יהודי','יהודים').str.replace('יהודי','יהודים')\n",
    "                            .str.replace('היהודים','יהודים').str.replace('היהודי','יהודים').str.replace('שיהודיה','יהודים')\n",
    "                            .str.replace('ליהודים','יהודים').str.replace('מהיהודים','יהודים').str.replace('יהודית','יהודים')\n",
    "                           .str.replace('יהודיות','יהודים'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('המנון','ההמנון').str.replace('ההימנון','ההמנון')\n",
    "                            .str.replace('בהמנון','ההמנון').str.replace('בהימנון','ההמנון').str.replace('המנונים','ההמנון')\n",
    "                            .str.replace('שבהמנון','ההמנון').str.replace('שבהימנון','ההמנון').str.replace('להמנון','ההמנון')\n",
    "                           .str.replace('להימנון','ההמנון'))\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('מוסלמים','מוסלמי').str.replace('המוסלמים','מוסלמי')\n",
    "                            .str.replace('ממוסלמי','מוסלמי').str.replace('למוסלמי','מוסלמי').str.replace('למוסלמים','מוסלמי')\n",
    "                            .str.replace('שמוסלמי','מוסלמי').str.replace('שמוסלמים','מוסלמי'))\n",
    "\n",
    "\n",
    "subset_1['clean_new'] = (subset_1['clean_new'].str.replace('גזענית','גזעני').str.replace('גזענות','גזעני')\n",
    "                            .str.replace('הגזענית','גזעני').str.replace('גזען','גזעני').str.replace('הגזען','גזעני')\n",
    "                            .str.replace('גזעניים','גזעני').str.replace('הגזענות','גזעני').str.replace('מהגזענים','גזעני')\n",
    "                           .str.replace('גזענים','גזעני').str.replace('הגזעניים','גזעני').str.replace('גזעי','גזעני')\n",
    "                           .str.replace('וגזעי','גזעני').str.replace('גיזענית','גזעני').str.replace('גיזעני','גזעני')\n",
    "                           .str.replace('הגיזענית','גזעני').str.replace('גיזען','גזעני').str.replace('גיזעני','גזעני')\n",
    "                           .str.replace('גיזעניים','גזעני').str.replace('הגיזענות','גזעני').str.replace('מהגיזענים','גזעני')\n",
    "                           .str.replace('גיזענים','גזעני').str.replace('הגיזעניים','גזעני').str.replace('גיזענות','גזעני'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5e0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data and create train and test sets\n",
    "def get_feature_vector(train_fit):\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(train_fit)\n",
    "    return vector\n",
    "\n",
    "tf_vector = get_feature_vector(np.array(subset_1.iloc[:, 5].values.astype('U')).ravel())\n",
    "X = tf_vector.transform(np.array(subset_1.iloc[:, 5].values.astype('U')).ravel())\n",
    "y = np.array(subset_1.iloc[:, 3]).ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# create folds for cross validation\n",
    "k = 10\n",
    "kf_10 = KFold(n_splits = k, random_state = 0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2adf53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logisitic regression\n",
    "\n",
    "lm_accuracy_test_list = []\n",
    "lm_f1_test_list = []\n",
    "\n",
    "LM_model = LogisticRegressionCV(penalty='l2', max_iter=500, multi_class='multinomial', solver='lbfgs',\n",
    "                                cv=10)\n",
    "LM_model.fit(X_train,y_train)\n",
    "\n",
    "#Target prediction & F1 score from test set.\n",
    "y_pred_test = LM_model.predict(X_test)\n",
    "lm_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "lm_accuracy_test_list.append(lm_accuracy_test)\n",
    "lm_f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "lm_f1_test_list.append(lm_f1_test)\n",
    "\n",
    "#print(lm_accuracy_test_list)\n",
    "#print(lm_f1_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df51c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive Bayes\n",
    "\n",
    "nb_accuracy_CV_list = []\n",
    "nb_f1_CV_list = []\n",
    "nb_accuracy_test_list = []\n",
    "nb_f1_test_list = []\n",
    "\n",
    "\n",
    "# Training Naive Bayes model\n",
    "NB_model = MultinomialNB()\n",
    "\n",
    "\n",
    "# cross validation\n",
    "for train_index, test_index in kf_10.split(X_train):\n",
    "    X_train_CV, X_test_CV = X[train_index], X[test_index]\n",
    "    y_train_CV, y_test_CV = y[train_index], y[test_index]\n",
    "    NB_model.fit(X_train_CV, y_train_CV)\n",
    "\n",
    "    #Target prediction, accuracy & F1 score from CV.\n",
    "    y_pred_CV = NB_model.predict(X_test_CV)\n",
    "    nb_accuracy_CV = accuracy_score(y_test_CV, y_pred_CV)\n",
    "    nb_accuracy_CV_list.append(nb_accuracy_CV)\n",
    "    nb_f1_CV = f1_score(y_test_CV, y_pred_CV, average = 'weighted')\n",
    "    nb_f1_CV_list.append(nb_f1_CV)\n",
    "\n",
    "    #Target prediction & F1 score  from test set.\n",
    "    y_pred_test = NB_model.predict(X_test)\n",
    "    nb_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    nb_accuracy_test_list.append(nb_accuracy_test)\n",
    "    nb_f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    nb_f1_test_list.append(nb_f1_test)\n",
    "    \n",
    "\n",
    "#print(np.mean(nb_accuracy_test_list))\n",
    "#print(np.mean(nb_f1_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ec92b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "\n",
    "max_depth = [5, 8, 12, 15, 20]\n",
    "max_leaf_nodes = [15, 18, 22, 25, 30]\n",
    "\n",
    "\n",
    "rfc_accuracy_CV_list = []\n",
    "rfc_f1_CV_list = []\n",
    "rfc_accuracy_test_list = []\n",
    "rfc_f1_test_list = []\n",
    "\n",
    "for i in max_depth:\n",
    "    for j in max_leaf_nodes:\n",
    "        # random forest model\n",
    "        rf_model = RandomForestClassifier(max_depth=i, max_leaf_nodes = j, random_state=0)\n",
    "        \n",
    "        # cross validation\n",
    "        for train_index, test_index in kf_10.split(X_train):\n",
    "            X_train_CV, X_test_CV = X[train_index], X[test_index]\n",
    "            y_train_CV, y_test_CV = y[train_index], y[test_index]\n",
    "            rf_model.fit(X_train_CV, y_train_CV)\n",
    "\n",
    "            #Target prediction, accuracy & F1 score from CV.\n",
    "            y_pred_CV = rf_model.predict(X_test_CV)\n",
    "            rfc_accuracy_CV = accuracy_score(y_test_CV, y_pred_CV)\n",
    "            rfc_accuracy_CV_list.append(rfc_accuracy_CV)\n",
    "            rfc_f1_CV = f1_score(y_test_CV, y_pred_CV, average = 'weighted')\n",
    "            rfc_f1_CV_list.append([rfc_f1_CV,i,j])\n",
    "\n",
    "            #Target prediction & F1 score  from test set.\n",
    "            y_pred_test = rf_model.predict(X_test)\n",
    "            rfc_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "            rfc_accuracy_test_list.append(rfc_accuracy_test)\n",
    "            rfc_f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "            rfc_f1_test_list.append([rfc_f1_test,i,j])\n",
    "            \n",
    "\n",
    "# print(sorted(rfc_accuracy_test_list)[-1::-1])\n",
    "# print(sorted(rfc_f1_test_list)[-1::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bf6f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - poly\n",
    "\n",
    "d = [0.5,1,1.5,2,2.5]\n",
    "C = [0.3,0.6,0.9,1.2,1.5]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "poly_accuracy_CV_list = []\n",
    "poly_f1_CV_list = []\n",
    "poly_accuracy_test_list = []\n",
    "poly_f1_test_list = []\n",
    "\n",
    "for i in d:\n",
    "    for j in C:\n",
    "        # svm poly model\n",
    "        model_poly = svm.SVC(kernel='poly', degree=i, C=j)\n",
    "\n",
    "        # cross validation\n",
    "    \n",
    "        for train_index, test_index in kf_10.split(X_train):\n",
    "            X_train_CV, X_test_CV = X[train_index], X[test_index]\n",
    "            y_train_CV, y_test_CV = y[train_index], y[test_index]\n",
    "            model_poly.fit(X_train_CV, y_train_CV)\n",
    "\n",
    "            #Target prediction, accuracy & F1 score from CV.\n",
    "            y_pred_CV = model_poly.predict(X_test_CV)\n",
    "            poly_accuracy_CV = accuracy_score(y_test_CV, y_pred_CV)\n",
    "            poly_accuracy_CV_list.append(poly_accuracy_CV)\n",
    "            poly_f1_CV = f1_score(y_test_CV, y_pred_CV, average = 'weighted')\n",
    "            poly_f1_CV_list.append([poly_f1_CV,i,j])\n",
    "\n",
    "            #Target prediction & F1 score  from test set.\n",
    "            y_pred_test = model_poly.predict(X_test)\n",
    "            poly_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "            poly_accuracy_test_list.append(poly_accuracy_test)\n",
    "            poly_f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "            poly_f1_test_list.append([poly_f1_test,i,j])\n",
    "\n",
    "            \n",
    "# print(sorted(poly_accuracy_test_list)[-1::-1])\n",
    "# print(sorted(poly_f1_test_list)[-1::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5409bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - rbf\n",
    "\n",
    "degree = [0.5,1,1.5,2,2.5]\n",
    "C = [0.3,0.6,0.9,1.2,1.5]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "rbf_accuracy_CV_list = []\n",
    "rbf_f1_CV_list = []\n",
    "rbf_accuracy_test_list = []\n",
    "rbf_f1_test_list = []\n",
    "\n",
    "for i in d:\n",
    "    for j in C:\n",
    "        # svm rbf model\n",
    "        model_rbf = svm.SVC(kernel='rbf', gamma=i, C=j)\n",
    "\n",
    "        # cross validation\n",
    "        for train_index, test_index in kf_10.split(X_train):\n",
    "            X_train_CV, X_test_CV = X[train_index], X[test_index]\n",
    "            y_train_CV, y_test_CV = y[train_index], y[test_index]\n",
    "            model_rbf.fit(X_train_CV, y_train_CV)\n",
    "\n",
    "            #Target prediction, accuracy & F1 score from CV.\n",
    "            y_pred_CV = model_rbf.predict(X_test_CV)\n",
    "            rbf_accuracy_CV = accuracy_score(y_test_CV, y_pred_CV)\n",
    "            rbf_accuracy_CV_list.append(rbf_accuracy_CV)\n",
    "            rbf_f1_CV = f1_score(y_test_CV, y_pred_CV, average = 'weighted')\n",
    "            rbf_f1_CV_list.append([rbf_f1_CV,i,j])\n",
    "\n",
    "            #Target prediction & F1 score  from test set.\n",
    "            y_pred_test = model_rbf.predict(X_test)\n",
    "            rbf_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "            rbf_accuracy_test_list.append(rbf_accuracy_test)\n",
    "            rbf_f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "            rbf_f1_test_list.append([rbf_f1_test,i,j])\n",
    "\n",
    "            \n",
    "# print(sorted(rbf_accuracy_test_list)[-1::-1])\n",
    "# print(sorted(rbf_f1_test_list)[-1::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Porject",
   "language": "python",
   "name": "porject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
